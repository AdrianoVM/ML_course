{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Warmup questions:\n",
    "- What does each column of X˜ represent? 1 is w0, 2 is input variable\n",
    "- What does each row of X˜ represent? samples\n",
    "- Why do we have 1’s in X˜ ? because it is to compensate for w0\n",
    "- If we have heights and weights of 3 people, what would be the size of y and X˜ ? What would X˜\n",
    "32 represent?  y(3,1) X(3,2)\n",
    "- In helpers.py, we have already provided code to form arrays for y and X˜ . Have a look at the code, and\n",
    "make sure you understand how they are constructed. ok\n",
    "- Check if the sizes of the variables make sense (use functions shape).\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "((10000,), (10000, 2))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "$L(w) = e^2/2N$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Computing the Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the `compute_loss` function below:\n",
    "<a id='compute_loss'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": "36.14696100105259"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_loss(y, tx, w, mae=False):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "    N = tx.shape[0]\n",
    "    if mae:\n",
    "        print(w)\n",
    "        e = y - tx @ np.asarray(w)\n",
    "        loss = np.sum(abs(e)) / (2*N)\n",
    "    else:\n",
    "        e = y - tx @ np.asarray(w)\n",
    "        loss = (e @ e.T) / (2* N)\n",
    "    return loss\n",
    "compute_loss(y, tx, np.array([1,2]).T, mae=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Grid Search\n",
    "\n",
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1):\n",
    "    \"\"\"Algorithm for grid search.\"\"\"\n",
    "    losses = np.zeros((len(w0), len(w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    # ***************************************************\n",
    "    for i in range(w0.shape[0]):\n",
    "        for j in range(w1.shape[0]):\n",
    "            losses[i,j] = compute_loss(y, tx, np.array((w0[i], w1[j])))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=18.79354101952324, w0*=71.42857142857142, w1*=15.306122448979579, execution time=0.117 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 720x432 with 3 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=50)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have issues caused by the granularity of the grid search. 10 * 10 is not a lot.\n",
    "\n",
    "10 fit: Grid Search: loss*=42.424483146782485, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.011 seconds\n",
    "\n",
    "50 fit: Grid Search: loss*=18.793541019523236, w0*=71.42857142857142, w1*=15.306122448979579, execution time=0.233 seconds\n",
    "\n",
    "more values make better results, but it's remarkably more expensive."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "89.60115491762572"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient and error vector\n",
    "    # ***************************************************\n",
    "    e = y - tx @ np.asarray(w)\n",
    "    N = tx.shape[0]\n",
    "    gradient = - 1/N * tx.T @ e\n",
    "    return gradient\n",
    "np.linalg.norm(compute_gradient(y,tx,np.array([50, 100])))\n",
    "# 100-20: [26.706078    6.52028757] norm 27.490521129292492\n",
    "# 50-10: [-23.293922    -3.47971243] norm 23.552392678247735"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The norm gives us the steepness of the curve, the vector is the direction to the minimum\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=2792.236712759168, w0=7.3293922002105205, w1=1.3479712434988973\n",
      "Gradient Descent(1/49): loss=2264.635056030003, w0=13.925845180399985, w1=2.561145362647904\n",
      "Gradient Descent(2/49): loss=1837.27771407938, w0=19.862652862570506, w1=3.6530020698820134\n",
      "Gradient Descent(3/49): loss=1491.1182670993755, w0=25.205779776523972, w1=4.6356731063927095\n",
      "Gradient Descent(4/49): loss=1210.729115045572, w0=30.01459399908209, w1=5.520077039252336\n",
      "Gradient Descent(5/49): loss=983.6139018819912, w0=34.3425267993844, w1=6.316040578826003\n",
      "Gradient Descent(6/49): loss=799.6505792194903, w0=38.23766631965648, w1=7.032407764442302\n",
      "Gradient Descent(7/49): loss=650.6402878628646, w0=41.74329188790136, w1=7.677138231496973\n",
      "Gradient Descent(8/49): loss=529.941951863998, w0=44.89835489932174, w1=8.257395651846178\n",
      "Gradient Descent(9/49): loss=432.176299704916, w0=47.73791160960008, w1=8.779627330160462\n",
      "Gradient Descent(10/49): loss=352.98612145605955, w0=50.293512648850594, w1=9.249635840643318\n",
      "Gradient Descent(11/49): loss=288.84207707448576, w0=52.59355358417606, w1=9.67264350007789\n",
      "Gradient Descent(12/49): loss=236.88540112541105, w0=54.66359042596897, w1=10.053350393569003\n",
      "Gradient Descent(13/49): loss=194.80049360666055, w0=56.52662358358259, w1=10.395986597711007\n",
      "Gradient Descent(14/49): loss=160.71171851647264, w0=58.20335342543485, w1=10.704359181438809\n",
      "Gradient Descent(15/49): loss=133.09981069342047, w0=59.71241028310188, w1=10.98189450679383\n",
      "Gradient Descent(16/49): loss=110.73416535674818, w0=61.07056145500221, w1=11.231676299613351\n",
      "Gradient Descent(17/49): loss=92.61799263404362, w0=62.29289750971251, w1=11.45647991315092\n",
      "Gradient Descent(18/49): loss=77.94389272865291, w0=63.392999958951776, w1=11.658803165334731\n",
      "Gradient Descent(19/49): loss=66.05787180528648, w0=64.38309216326712, w1=11.840894092300163\n",
      "Gradient Descent(20/49): loss=56.43019485735959, w0=65.27417514715093, w1=12.004775926569051\n",
      "Gradient Descent(21/49): loss=48.63177652953886, w0=66.07614983264635, w1=12.15226957741105\n",
      "Gradient Descent(22/49): loss=42.315057684004074, w0=66.79792704959223, w1=12.285013863168848\n",
      "Gradient Descent(23/49): loss=37.198515419120916, w0=67.44752654484353, w1=12.404483720350868\n",
      "Gradient Descent(24/49): loss=33.05411618456553, w0=68.03216609056969, w1=12.512006591814686\n",
      "Gradient Descent(25/49): loss=29.697152804575687, w0=68.55834168172323, w1=12.608777176132122\n",
      "Gradient Descent(26/49): loss=26.978012466783916, w0=69.03189971376143, w1=12.695870702017814\n",
      "Gradient Descent(27/49): loss=24.77550879317254, w0=69.4581019425958, w1=12.774254875314936\n",
      "Gradient Descent(28/49): loss=22.991480817547366, w0=69.84168394854674, w1=12.844800631282347\n",
      "Gradient Descent(29/49): loss=21.546418157290947, w0=70.18690775390259, w1=12.908291811653017\n",
      "Gradient Descent(30/49): loss=20.375917402483253, w0=70.49760917872285, w1=12.965433873986619\n",
      "Gradient Descent(31/49): loss=19.42781179108901, w0=70.77724046106108, w1=13.016861730086863\n",
      "Gradient Descent(32/49): loss=18.65984624585969, w0=71.02890861516549, w1=13.063146800577082\n",
      "Gradient Descent(33/49): loss=18.037794154223942, w0=71.25540995385946, w1=13.104803364018277\n",
      "Gradient Descent(34/49): loss=17.53393195999899, w0=71.45926115868403, w1=13.142294271115354\n",
      "Gradient Descent(35/49): loss=17.125803582676767, w0=71.64272724302614, w1=13.176036087502723\n",
      "Gradient Descent(36/49): loss=16.795219597045776, w0=71.80784671893404, w1=13.206403722251356\n",
      "Gradient Descent(37/49): loss=16.527446568684667, w0=71.95645424725116, w1=13.233734593525124\n",
      "Gradient Descent(38/49): loss=16.310550415712164, w0=72.09020102273657, w1=13.258332377671517\n",
      "Gradient Descent(39/49): loss=16.134864531804435, w0=72.21057312067343, w1=13.28047038340327\n",
      "Gradient Descent(40/49): loss=15.992558965839175, w0=72.3189080088166, w1=13.300394588561847\n",
      "Gradient Descent(41/49): loss=15.877291457407319, w0=72.41640940814547, w1=13.318326373204567\n",
      "Gradient Descent(42/49): loss=15.783924775577509, w0=72.50416066754144, w1=13.334464979383014\n",
      "Gradient Descent(43/49): loss=15.708297763295374, w0=72.58313680099782, w1=13.348989724943618\n",
      "Gradient Descent(44/49): loss=15.647039883346832, w0=72.65421532110855, w1=13.36206199594816\n",
      "Gradient Descent(45/49): loss=15.597421000588522, w0=72.71818598920821, w1=13.373827039852248\n",
      "Gradient Descent(46/49): loss=15.557229705554297, w0=72.77575959049791, w1=13.384415579365928\n",
      "Gradient Descent(47/49): loss=15.524674756576562, w0=72.82757583165863, w1=13.39394526492824\n",
      "Gradient Descent(48/49): loss=15.498305247904606, w0=72.8742104487033, w1=13.40252198193432\n",
      "Gradient Descent(49/49): loss=15.476945945880312, w0=72.91618160404349, w1=13.410241027239794\n",
      "Gradient Descent: execution time=0.016 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fcc94595826b4115a2ec50186ef8f7ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<function __main__.plot_figure(n_iter)>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation.It's same as the gradient descent.\n",
    "    # ***************************************************\n",
    "    e = y - tx @ np.asarray(w)\n",
    "    N = tx.shape[0]\n",
    "    gradient = - 1/N * tx.T @ e\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient descent.\n",
    "    # ***************************************************\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        small_y, small_tx = batch_iter(y, tx, batch_size).__next__()\n",
    "        small_y, small_tx = np.asarray(small_y), np.asarray(small_tx)\n",
    "        #small_y = np.zeros([0])\n",
    "        #for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "        #    small_y = np.append(small_y,minibatch_y)\n",
    "        #    print(minibatch_y)\n",
    "        #print(small_y)\n",
    "        loss = compute_loss(small_y, small_tx, w)\n",
    "        gradient = compute_stoch_gradient(small_y, small_tx, w)\n",
    "\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/49): loss=3461.449525038572, w0=57.23362912885602, w1=31.056159108223618\n",
      "Stochastic Gradient Descent(1/49): loss=305.6729343269742, w0=68.9088268608575, w1=22.273433858718953\n",
      "Stochastic Gradient Descent(2/49): loss=23.38562833304363, w0=71.6746578858043, w1=20.457983318369045\n",
      "Stochastic Gradient Descent(3/49): loss=13.231498195471678, w0=74.4894148734279, w1=20.675881361644095\n",
      "Stochastic Gradient Descent(4/49): loss=44.46539707140182, w0=68.38377227761464, w1=12.211558835651788\n",
      "Stochastic Gradient Descent(5/49): loss=17.734436980425397, w0=72.29140332462165, w1=13.770296247449789\n",
      "Stochastic Gradient Descent(6/49): loss=0.626333200178076, w0=72.15688039406966, w1=13.75994313401602\n",
      "Stochastic Gradient Descent(7/49): loss=5.385339838230333, w0=69.96736003742137, w1=15.546951952796046\n",
      "Stochastic Gradient Descent(8/49): loss=11.667236837428815, w0=69.61692207941644, w1=12.523878427622865\n",
      "Stochastic Gradient Descent(9/49): loss=2.0500369796982665, w0=70.37877453991955, w1=12.62799992262692\n",
      "Stochastic Gradient Descent(10/49): loss=2.053485096575805, w0=69.89384621189794, w1=13.663986770712338\n",
      "Stochastic Gradient Descent(11/49): loss=1.2429216430082461, w0=70.06908843147525, w1=13.354836277805632\n",
      "Stochastic Gradient Descent(12/49): loss=39.34890412149678, w0=70.97338675554784, w1=13.541514289555753\n",
      "Stochastic Gradient Descent(13/49): loss=1.1157919272708983, w0=70.81135140018345, w1=12.740494518998313\n",
      "Stochastic Gradient Descent(14/49): loss=25.129659637537213, w0=71.63078849508483, w1=15.685744630477508\n",
      "Stochastic Gradient Descent(15/49): loss=6.444765853775377, w0=70.21034261546323, w1=15.849381675304295\n",
      "Stochastic Gradient Descent(16/49): loss=53.5042139670687, w0=76.22508776797942, w1=16.6995870074679\n",
      "Stochastic Gradient Descent(17/49): loss=12.857423863294192, w0=75.87259777998794, w1=18.57305181217814\n",
      "Stochastic Gradient Descent(18/49): loss=23.093189326641106, w0=73.3232121368537, w1=15.418789586048815\n",
      "Stochastic Gradient Descent(19/49): loss=12.346672925118275, w0=72.3112266170723, w1=15.242618894203586\n",
      "Stochastic Gradient Descent(20/49): loss=16.794562871977792, w0=71.73895982200563, w1=15.284683987870553\n",
      "Stochastic Gradient Descent(21/49): loss=8.011562461703198, w0=72.45242387525514, w1=18.477488191156095\n",
      "Stochastic Gradient Descent(22/49): loss=13.377372495113898, w0=73.21454532558512, w1=19.90114662669877\n",
      "Stochastic Gradient Descent(23/49): loss=22.602005556778085, w0=76.62706730933964, w1=16.102144253643846\n",
      "Stochastic Gradient Descent(24/49): loss=2.571836313160921, w0=75.4683929840638, w1=14.382890797494106\n",
      "Stochastic Gradient Descent(25/49): loss=4.214083692291718, w0=75.27840741003976, w1=14.489936849408473\n",
      "Stochastic Gradient Descent(26/49): loss=22.64307791294846, w0=72.87719474288927, w1=13.994638480506225\n",
      "Stochastic Gradient Descent(27/49): loss=7.3106073375395875, w0=73.20178695304466, w1=16.845029867235006\n",
      "Stochastic Gradient Descent(28/49): loss=7.686633757589692, w0=73.00428748681144, w1=14.43859364129873\n",
      "Stochastic Gradient Descent(29/49): loss=48.96240485289963, w0=69.33276097823666, w1=10.411348339216106\n",
      "Stochastic Gradient Descent(30/49): loss=17.488376210642034, w0=70.89753305610108, w1=11.203908316814038\n",
      "Stochastic Gradient Descent(31/49): loss=1.4975445338391353, w0=70.14458364093375, w1=12.068619228716418\n",
      "Stochastic Gradient Descent(32/49): loss=9.082090019984888, w0=72.13726893200634, w1=10.301047192586864\n",
      "Stochastic Gradient Descent(33/49): loss=30.690346489408757, w0=72.53617010521904, w1=11.262776375400922\n",
      "Stochastic Gradient Descent(34/49): loss=5.436086166017419, w0=72.84122607261943, w1=12.314735763847318\n",
      "Stochastic Gradient Descent(35/49): loss=15.063918031691538, w0=74.12394732271889, w1=12.24658678770629\n",
      "Stochastic Gradient Descent(36/49): loss=3.8157337493323698, w0=75.22022783388348, w1=12.46337738253194\n",
      "Stochastic Gradient Descent(37/49): loss=9.629635910217376, w0=72.82521594902539, w1=13.6161578594814\n",
      "Stochastic Gradient Descent(38/49): loss=21.89331985589602, w0=69.12051763990107, w1=16.490325997129\n",
      "Stochastic Gradient Descent(39/49): loss=15.54919091528671, w0=70.51212923880198, w1=15.884072179169458\n",
      "Stochastic Gradient Descent(40/49): loss=54.03603364054067, w0=77.14681855198869, w1=5.135146863612482\n",
      "Stochastic Gradient Descent(41/49): loss=183.64193293508643, w0=71.08160635255561, w1=17.95315520744215\n",
      "Stochastic Gradient Descent(42/49): loss=30.389428419133818, w0=76.44555904504696, w1=14.651960348371915\n",
      "Stochastic Gradient Descent(43/49): loss=8.63471222865409, w0=74.37276943309017, w1=13.441534454938274\n",
      "Stochastic Gradient Descent(44/49): loss=4.7196185732861915, w0=73.33015256628337, w1=11.125459857133693\n",
      "Stochastic Gradient Descent(45/49): loss=3.972989448612807, w0=73.97760701281459, w1=12.458469506977062\n",
      "Stochastic Gradient Descent(46/49): loss=25.807758051434522, w0=76.81398188342409, w1=13.034152804770894\n",
      "Stochastic Gradient Descent(47/49): loss=65.53675133281344, w0=69.19219553935935, w1=10.002986490301016\n",
      "Stochastic Gradient Descent(48/49): loss=8.077196166579775, w0=71.47953630454693, w1=11.387604546562628\n",
      "Stochastic Gradient Descent(49/49): loss=6.402414558663566, w0=72.51528787943573, w1=10.52977653890404\n",
      "SGD: execution time=0.035 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "batch_size = 3\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91183d1bfc5e40ffa877ce66bcf2aa46"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<function __main__.plot_figure(n_iter)>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Effect of Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=2869.835114535854, w0=7.406780585492638, w1=1.1034894865989184\n",
      "Gradient Descent(1/49): loss=2337.0932814935354, w0=14.072883112436012, w1=2.096630024537945\n",
      "Gradient Descent(2/49): loss=1905.572396729258, w0=20.072375386685046, w1=2.9904565086830694\n",
      "Gradient Descent(3/49): loss=1556.0404800701936, w0=25.471918433509178, w1=3.794900344413678\n",
      "Gradient Descent(4/49): loss=1272.9196275763516, w0=30.3315071756509, w1=4.518899796571227\n",
      "Gradient Descent(5/49): loss=1043.5917370563393, w0=34.705137043578446, w1=5.170499303513019\n",
      "Gradient Descent(6/49): loss=857.8361457351293, w0=38.64140392471324, w1=5.7569388597606315\n",
      "Gradient Descent(7/49): loss=707.3741167649495, w0=42.18404411773455, w1=6.284734460383483\n",
      "Gradient Descent(8/49): loss=585.4998732991037, w0=45.372420291453736, w1=6.759750500944048\n",
      "Gradient Descent(9/49): loss=486.7817360917684, w0=48.241958847801, w1=7.187264937448557\n",
      "Gradient Descent(10/49): loss=406.82004495382694, w0=50.82454354851354, w1=7.572027930302614\n",
      "Gradient Descent(11/49): loss=342.0510751320945, w0=53.14886977915482, w1=7.918314623871265\n",
      "Gradient Descent(12/49): loss=289.5882095764912, w0=55.24076338673198, w1=8.22997264808305\n",
      "Gradient Descent(13/49): loss=247.0932884764524, w0=57.123467633551414, w1=8.510464869873658\n",
      "Gradient Descent(14/49): loss=212.67240238542118, w0=58.81790145568891, w1=8.762907869485204\n",
      "Gradient Descent(15/49): loss=184.79148465168572, w0=60.34289189561266, w1=8.990106569135595\n",
      "Gradient Descent(16/49): loss=162.20794128736, w0=61.71538329154403, w1=9.194585398820948\n",
      "Gradient Descent(17/49): loss=143.91527116225635, w0=62.95062554788226, w1=9.378616345537765\n",
      "Gradient Descent(18/49): loss=129.09820836092226, w0=64.06234357858668, w1=9.544244197582898\n",
      "Gradient Descent(19/49): loss=117.09638749184161, w0=65.06288980622065, w1=9.69330926442352\n",
      "Gradient Descent(20/49): loss=107.3749125878863, w0=65.96338141109122, w1=9.82746782458008\n",
      "Gradient Descent(21/49): loss=99.50051791568256, w0=66.77382385547473, w1=9.948210528720983\n",
      "Gradient Descent(22/49): loss=93.12225823119755, w0=67.5032220554199, w1=10.056878962447795\n",
      "Gradient Descent(23/49): loss=87.95586788676464, w0=68.15968043537055, w1=10.154680552801928\n",
      "Gradient Descent(24/49): loss=83.77109170777399, w0=68.75049297732613, w1=10.242701984120647\n",
      "Gradient Descent(25/49): loss=80.38142300279155, w0=69.28222426508616, w1=10.321921272307492\n",
      "Gradient Descent(26/49): loss=77.63579135175578, w0=69.76078242407017, w1=10.393218631675653\n",
      "Gradient Descent(27/49): loss=75.41182971441684, w0=70.19148476715579, w1=10.457386255106998\n",
      "Gradient Descent(28/49): loss=73.61042078817232, w0=70.57911687593284, w1=10.51513711619521\n",
      "Gradient Descent(29/49): loss=72.15127955791425, w0=70.92798577383219, w1=10.567112891174599\n",
      "Gradient Descent(30/49): loss=70.96937516140518, w0=71.24196778194161, w1=10.61389108865605\n",
      "Gradient Descent(31/49): loss=70.01203260023281, w0=71.52455158924009, w1=10.655991466389356\n",
      "Gradient Descent(32/49): loss=69.2365851256832, w0=71.77887701580872, w1=10.693881806349331\n",
      "Gradient Descent(33/49): loss=68.60847267129806, w0=72.00776989972049, w1=10.727983112313309\n",
      "Gradient Descent(34/49): loss=68.09970158324603, w0=72.21377349524107, w1=10.758674287680888\n",
      "Gradient Descent(35/49): loss=67.68759700192396, w0=72.39917673120961, w1=10.78629634551171\n",
      "Gradient Descent(36/49): loss=67.35379229105303, w0=72.56603964358128, w1=10.811156197559448\n",
      "Gradient Descent(37/49): loss=67.0834104752476, w0=72.71621626471578, w1=10.833530064402414\n",
      "Gradient Descent(38/49): loss=66.8644012044452, w0=72.85137522373684, w1=10.853666544561083\n",
      "Gradient Descent(39/49): loss=66.68700369509526, w0=72.97301828685579, w1=10.871789376703886\n",
      "Gradient Descent(40/49): loss=66.54331171252181, w0=73.08249704366285, w1=10.888099925632407\n",
      "Gradient Descent(41/49): loss=66.4269212066373, w0=73.1810279247892, w1=10.902779419668077\n",
      "Gradient Descent(42/49): loss=66.33264489687086, w0=73.26970571780292, w1=10.91599096430018\n",
      "Gradient Descent(43/49): loss=66.25628108596004, w0=73.34951573151527, w1=10.927881354469072\n",
      "Gradient Descent(44/49): loss=66.19442639912226, w0=73.42134474385638, w1=10.938582705621075\n",
      "Gradient Descent(45/49): loss=66.14432410278367, w0=73.48599085496338, w1=10.948213921657878\n",
      "Gradient Descent(46/49): loss=66.10374124274942, w0=73.54417235495967, w1=10.956882016091\n",
      "Gradient Descent(47/49): loss=66.07086912612168, w0=73.59653570495634, w1=10.964683301080811\n",
      "Gradient Descent(48/49): loss=66.0442427116532, w0=73.64366271995334, w1=10.97170445757164\n",
      "Gradient Descent(49/49): loss=66.02267531593374, w0=73.68607703345064, w1=10.978023498413386\n",
      "Gradient Descent: execution time=0.000 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "218654003b554918aa7308ee6543c3dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def computeGD():\n",
    "    from plots import gradient_descent_visualization\n",
    "\n",
    "    # Define the parameters of the algorithm.\n",
    "    max_iters = 50\n",
    "    gamma = 0.1\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.array([0, 0])\n",
    "\n",
    "    # Start gradient descent.\n",
    "    start_time = datetime.datetime.now()\n",
    "    gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    # Print result\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    def plot_figure(n_iter):\n",
    "        fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "        fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "    interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))\n",
    "computeGD()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Subgradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the function `compute_loss(y, tx, w)` for the Mean Absolute Error cost function [here](#compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1.00000000e+00, -1.12561225e-15])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_subgradient(y, tx, w):\n",
    "    e = y - tx @ np.asarray(w)\n",
    "    N = tx.shape[0]\n",
    "    gradient = np.mean(np.array([-np.sign(e), -(np.sign(e)) * tx[:,1]]),axis=1)\n",
    "    # gradient = np.array([-np.sum(np.sign(e)), -(np.sign(e)) @ tx[:,1]]) * 1/N\n",
    "    return gradient\n",
    "compute_subgradient(y, tx, w_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def gradient_descent_sub(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y, tx, w, True)\n",
    "        gradient = compute_subgradient(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "Gradient Descent(0/99): loss=37.03390292746319, w0=1.3, w1=1.4632959310703054e-15\n",
      "[1.30000000e+00 1.46329593e-15]\n",
      "Gradient Descent(1/99): loss=36.38390292746319, w0=2.6, w1=2.926591862140611e-15\n",
      "[2.60000000e+00 2.92659186e-15]\n",
      "Gradient Descent(2/99): loss=35.733902927463184, w0=3.9000000000000004, w1=4.389887793210917e-15\n",
      "[3.90000000e+00 4.38988779e-15]\n",
      "Gradient Descent(3/99): loss=35.08390292746319, w0=5.2, w1=5.853183724281222e-15\n",
      "[5.20000000e+00 5.85318372e-15]\n",
      "Gradient Descent(4/99): loss=34.43390292746319, w0=6.5, w1=7.316479655351527e-15\n",
      "[6.50000000e+00 7.31647966e-15]\n",
      "Gradient Descent(5/99): loss=33.78390292746319, w0=7.8, w1=8.779775586421832e-15\n",
      "[7.80000000e+00 8.77977559e-15]\n",
      "Gradient Descent(6/99): loss=33.13390292746319, w0=9.1, w1=1.0243071517492137e-14\n",
      "[9.10000000e+00 1.02430715e-14]\n",
      "Gradient Descent(7/99): loss=32.483902927463184, w0=10.4, w1=1.1706367448562442e-14\n",
      "[1.04000000e+01 1.17063674e-14]\n",
      "Gradient Descent(8/99): loss=31.83390292746319, w0=11.700000000000001, w1=1.3169663379632747e-14\n",
      "[1.17000000e+01 1.31696634e-14]\n",
      "Gradient Descent(9/99): loss=31.183902927463187, w0=13.000000000000002, w1=1.4632959310703054e-14\n",
      "[1.30000000e+01 1.46329593e-14]\n",
      "Gradient Descent(10/99): loss=30.533902927463192, w0=14.300000000000002, w1=1.609625524177336e-14\n",
      "[1.43000000e+01 1.60962552e-14]\n",
      "Gradient Descent(11/99): loss=29.883902927463186, w0=15.600000000000003, w1=1.7559551172843667e-14\n",
      "[1.56000000e+01 1.75595512e-14]\n",
      "Gradient Descent(12/99): loss=29.233902927463184, w0=16.900000000000002, w1=1.9022847103913974e-14\n",
      "[1.69000000e+01 1.90228471e-14]\n",
      "Gradient Descent(13/99): loss=28.58390292746319, w0=18.200000000000003, w1=2.048614303498428e-14\n",
      "[1.8200000e+01 2.0486143e-14]\n",
      "Gradient Descent(14/99): loss=27.933902927463187, w0=19.500000000000004, w1=2.1949438966054587e-14\n",
      "[1.9500000e+01 2.1949439e-14]\n",
      "Gradient Descent(15/99): loss=27.28390292746319, w0=20.800000000000004, w1=2.3412734897124894e-14\n",
      "[2.08000000e+01 2.34127349e-14]\n",
      "Gradient Descent(16/99): loss=26.633902927463186, w0=22.100000000000005, w1=2.48760308281952e-14\n",
      "[2.21000000e+01 2.48760308e-14]\n",
      "Gradient Descent(17/99): loss=25.98390292746319, w0=23.400000000000006, w1=2.6339326759265507e-14\n",
      "[2.34000000e+01 2.63393268e-14]\n",
      "Gradient Descent(18/99): loss=25.333902927463186, w0=24.700000000000006, w1=2.7802622690335813e-14\n",
      "[2.47000000e+01 2.78026227e-14]\n",
      "Gradient Descent(19/99): loss=24.683902927463187, w0=26.000000000000007, w1=2.926591862140612e-14\n",
      "[2.60000000e+01 2.92659186e-14]\n",
      "Gradient Descent(20/99): loss=24.03390292746318, w0=27.300000000000008, w1=3.0729214552476427e-14\n",
      "[2.73000000e+01 3.07292146e-14]\n",
      "Gradient Descent(21/99): loss=23.383902927463186, w0=28.60000000000001, w1=3.2192510483546733e-14\n",
      "[2.86000000e+01 3.21925105e-14]\n",
      "Gradient Descent(22/99): loss=22.733902927463184, w0=29.90000000000001, w1=3.365580641461704e-14\n",
      "[2.99000000e+01 3.36558064e-14]\n",
      "Gradient Descent(23/99): loss=22.083902927463186, w0=31.20000000000001, w1=3.5119102345687347e-14\n",
      "[3.12000000e+01 3.51191023e-14]\n",
      "Gradient Descent(24/99): loss=21.433902927463183, w0=32.50000000000001, w1=3.6582398276757653e-14\n",
      "[3.25000000e+01 3.65823983e-14]\n",
      "Gradient Descent(25/99): loss=20.78390292746319, w0=33.800000000000004, w1=3.804569420782796e-14\n",
      "[3.38000000e+01 3.80456942e-14]\n",
      "Gradient Descent(26/99): loss=20.133902927463186, w0=35.1, w1=3.9508990138898266e-14\n",
      "[3.51000000e+01 3.95089901e-14]\n",
      "Gradient Descent(27/99): loss=19.48390292746319, w0=36.4, w1=4.097228606996857e-14\n",
      "[3.64000000e+01 4.09722861e-14]\n",
      "Gradient Descent(28/99): loss=18.83390292746319, w0=37.699999999999996, w1=4.243558200103888e-14\n",
      "[3.7700000e+01 4.2435582e-14]\n",
      "Gradient Descent(29/99): loss=18.18390292746319, w0=38.99999999999999, w1=4.3898877932109186e-14\n",
      "[3.90000000e+01 4.38988779e-14]\n",
      "Gradient Descent(30/99): loss=17.533902927463192, w0=40.29999999999999, w1=4.536217386317949e-14\n",
      "[4.03000000e+01 4.53621739e-14]\n",
      "Gradient Descent(31/99): loss=16.883902927463197, w0=41.59999999999999, w1=4.68254697942498e-14\n",
      "[4.16000000e+01 4.68254698e-14]\n",
      "Gradient Descent(32/99): loss=16.2339029274632, w0=42.899999999999984, w1=4.8288765725320106e-14\n",
      "[4.29000000e+01 4.82887657e-14]\n",
      "Gradient Descent(33/99): loss=15.583902927463198, w0=44.19999999999998, w1=4.975206165639041e-14\n",
      "[4.42000000e+01 4.97520617e-14]\n",
      "Gradient Descent(34/99): loss=14.9339029274632, w0=45.49999999999998, w1=5.121535758746072e-14\n",
      "[4.55000000e+01 5.12153576e-14]\n",
      "Gradient Descent(35/99): loss=14.283902927463199, w0=46.799999999999976, w1=5.2678653518531026e-14\n",
      "[4.68000000e+01 5.26786535e-14]\n",
      "Gradient Descent(36/99): loss=13.63614005532954, w0=48.08712871287126, w1=0.02070314197390876\n",
      "[4.80871287e+01 2.07031420e-02]\n",
      "Gradient Descent(37/99): loss=13.00293802541276, w0=49.36138613861384, w1=0.061444947738361545\n",
      "[49.36138614  0.06144495]\n",
      "Gradient Descent(38/99): loss=12.390654383002566, w0=50.584158415841564, w1=0.17674087327880902\n",
      "[50.58415842  0.17674087]\n",
      "Gradient Descent(39/99): loss=11.812614961055377, w0=51.78118811881186, w1=0.3306553437375094\n",
      "[51.78118812  0.33065534]\n",
      "Gradient Descent(40/99): loss=11.256949620950104, w0=52.95247524752473, w1=0.5154874931996253\n",
      "[52.95247525  0.51548749]\n",
      "Gradient Descent(41/99): loss=10.723232803992092, w0=54.08514851485146, w1=0.7475807766461836\n",
      "[54.08514851  0.74758078]\n",
      "Gradient Descent(42/99): loss=10.215033252399362, w0=55.19207920792077, w1=1.0077043892667585\n",
      "[55.19207921  1.00770439]\n",
      "Gradient Descent(43/99): loss=9.723134777200126, w0=56.24752475247523, w1=1.321013354073866\n",
      "[56.24752475  1.32101335]\n",
      "Gradient Descent(44/99): loss=9.26614996305638, w0=57.23861386138612, w1=1.6834243633273984\n",
      "[57.23861386  1.68342436]\n",
      "Gradient Descent(45/99): loss=8.840893254506472, w0=58.17821782178216, w1=2.0915196528913413\n",
      "[58.17821782  2.09151965]\n",
      "Gradient Descent(46/99): loss=8.440820506960565, w0=59.092079207920776, w1=2.5165263895813994\n",
      "[59.09207921  2.51652639]\n",
      "Gradient Descent(47/99): loss=8.057613772005482, w0=59.96732673267325, w1=2.9663751521177018\n",
      "[59.96732673  2.96637515]\n",
      "Gradient Descent(48/99): loss=7.697911711329398, w0=60.739603960396025, w1=3.478125556531264\n",
      "[60.73960396  3.47812556]\n",
      "Gradient Descent(49/99): loss=7.369288821422034, w0=61.49900990099008, w1=3.996960393510973\n",
      "[61.4990099   3.99696039]\n",
      "Gradient Descent(50/99): loss=7.045337958658993, w0=62.23267326732672, w1=4.531831642945308\n",
      "[62.23267327  4.53183164]\n",
      "Gradient Descent(51/99): loss=6.731221358869224, w0=62.927722772277214, w1=5.07744465840622\n",
      "[62.92772277  5.07744466]\n",
      "Gradient Descent(52/99): loss=6.433005964721667, w0=63.59702970297028, w1=5.640859317812645\n",
      "[63.5970297   5.64085932]\n",
      "Gradient Descent(53/99): loss=6.1387728034372255, w0=64.25346534653464, w1=6.209469340229142\n",
      "[64.25346535  6.20946934]\n",
      "Gradient Descent(54/99): loss=5.851889470415506, w0=64.88415841584157, w1=6.792551162813907\n",
      "[64.88415842  6.79255116]\n",
      "Gradient Descent(55/99): loss=5.568136332146056, w0=65.5148514851485, w1=7.3756329853986715\n",
      "[65.51485149  7.37563299]\n",
      "Gradient Descent(56/99): loss=5.289095562725286, w0=66.08118811881187, w1=7.980646442031192\n",
      "[66.08118812  7.98064644]\n",
      "Gradient Descent(57/99): loss=5.02519783633313, w0=66.63465346534652, w1=8.593489064031152\n",
      "[66.63465347  8.59348906]\n",
      "Gradient Descent(58/99): loss=4.763879141098756, w0=67.17524752475246, w1=9.212890995420965\n",
      "[67.17524752  9.212891  ]\n",
      "Gradient Descent(59/99): loss=4.507656405081496, w0=67.69009900990098, w1=9.829714850843683\n",
      "[67.69009901  9.82971485]\n",
      "Gradient Descent(60/99): loss=4.260185020791687, w0=68.17920792079207, w1=10.431752097745306\n",
      "[68.17920792 10.4317521 ]\n",
      "Gradient Descent(61/99): loss=4.03468969721786, w0=68.66831683168316, w1=10.982334819291662\n",
      "[68.66831683 10.98233482]\n",
      "Gradient Descent(62/99): loss=3.830576568696347, w0=69.15742574257425, w1=11.507362979139321\n",
      "[69.15742574 11.50736298]\n",
      "Gradient Descent(63/99): loss=3.6366748778963895, w0=69.6079207920792, w1=12.034431356755332\n",
      "[69.60792079 12.03443136]\n",
      "Gradient Descent(64/99): loss=3.461169109258622, w0=70.01980198019801, w1=12.523480591260476\n",
      "[70.01980198 12.52348059]\n",
      "Gradient Descent(65/99): loss=3.304592865911627, w0=70.43168316831682, w1=12.978674940692558\n",
      "[70.43168317 12.97867494]\n",
      "Gradient Descent(66/99): loss=3.1609315462999845, w0=70.84356435643564, w1=13.42597068423222\n",
      "[70.84356436 13.42597068]\n",
      "Gradient Descent(67/99): loss=3.025679395882761, w0=71.21683168316831, w1=13.839091572986087\n",
      "[71.21683168 13.83909157]\n",
      "Gradient Descent(68/99): loss=2.9108434412287334, w0=71.56435643564356, w1=14.230210646530976\n",
      "[71.56435644 14.23021065]\n",
      "Gradient Descent(69/99): loss=2.8134015626722495, w0=71.83465346534653, w1=14.563055671457576\n",
      "[71.83465347 14.56305567]\n",
      "Gradient Descent(70/99): loss=2.751201163008786, w0=72.01485148514851, w1=14.81361881313203\n",
      "[72.01485149 14.81361881]\n",
      "Gradient Descent(71/99): loss=2.7206947576030416, w0=72.15643564356435, w1=15.011643843991271\n",
      "[72.15643564 15.01164384]\n",
      "Gradient Descent(72/99): loss=2.699443461783876, w0=72.28514851485149, w1=15.190216401033316\n",
      "[72.28514851 15.1902164 ]\n",
      "Gradient Descent(73/99): loss=2.6829941101338406, w0=72.37524752475248, w1=15.330186224964597\n",
      "[72.37524752 15.33018622]\n",
      "Gradient Descent(74/99): loss=2.672365765843016, w0=72.45247524752476, w1=15.454688620060065\n",
      "[72.45247525 15.45468862]\n",
      "Gradient Descent(75/99): loss=2.665232262563544, w0=72.4910891089109, w1=15.548867219843077\n",
      "[72.49108911 15.54886722]\n",
      "Gradient Descent(76/99): loss=2.6613883137383514, w0=72.51683168316832, w1=15.616162969600007\n",
      "[72.51683168 15.61616297]\n",
      "Gradient Descent(77/99): loss=2.6594828729525957, w0=72.55544554455446, w1=15.666109943846353\n",
      "[72.55544554 15.66610994]\n",
      "Gradient Descent(78/99): loss=2.6583196593214837, w0=72.5940594059406, w1=15.689768452690997\n",
      "[72.59405941 15.68976845]\n",
      "Gradient Descent(79/99): loss=2.657530907270753, w0=72.63267326732674, w1=15.71342696153564\n",
      "[72.63267327 15.71342696]\n",
      "Gradient Descent(80/99): loss=2.6567421552200225, w0=72.67128712871288, w1=15.737085470380284\n",
      "[72.67128713 15.73708547]\n",
      "Gradient Descent(81/99): loss=2.656148874008925, w0=72.67128712871288, w1=15.774335541474558\n",
      "[72.67128713 15.77433554]\n",
      "Gradient Descent(82/99): loss=2.6558429069969485, w0=72.6841584158416, w1=15.79500680148345\n",
      "[72.68415842 15.7950068 ]\n",
      "Gradient Descent(83/99): loss=2.655772236951003, w0=72.6841584158416, w1=15.80234671429142\n",
      "[72.68415842 15.80234671]\n",
      "Gradient Descent(84/99): loss=2.655751516058684, w0=72.6841584158416, w1=15.809686627099392\n",
      "[72.68415842 15.80968663]\n",
      "Gradient Descent(85/99): loss=2.655730795166366, w0=72.6841584158416, w1=15.817026539907364\n",
      "[72.68415842 15.81702654]\n",
      "Gradient Descent(86/99): loss=2.6557100742740474, w0=72.6841584158416, w1=15.824366452715335\n",
      "[72.68415842 15.82436645]\n",
      "Gradient Descent(87/99): loss=2.655689353381728, w0=72.6841584158416, w1=15.831706365523306\n",
      "[72.68415842 15.83170637]\n",
      "Gradient Descent(88/99): loss=2.6556686324894097, w0=72.6841584158416, w1=15.839046278331278\n",
      "[72.68415842 15.83904628]\n",
      "Gradient Descent(89/99): loss=2.6556479115970912, w0=72.6841584158416, w1=15.84638619113925\n",
      "[72.68415842 15.84638619]\n",
      "Gradient Descent(90/99): loss=2.655627190704772, w0=72.6841584158416, w1=15.85372610394722\n",
      "[72.68415842 15.8537261 ]\n",
      "Gradient Descent(91/99): loss=2.6556064698124535, w0=72.6841584158416, w1=15.861066016755192\n",
      "[72.68415842 15.86106602]\n",
      "Gradient Descent(92/99): loss=2.655585748920135, w0=72.6841584158416, w1=15.868405929563163\n",
      "[72.68415842 15.86840593]\n",
      "Gradient Descent(93/99): loss=2.6555650280278167, w0=72.6841584158416, w1=15.875745842371135\n",
      "[72.68415842 15.87574584]\n",
      "Gradient Descent(94/99): loss=2.6555443071354974, w0=72.6841584158416, w1=15.883085755179106\n",
      "[72.68415842 15.88308576]\n",
      "Gradient Descent(95/99): loss=2.6555235862431785, w0=72.6841584158416, w1=15.890425667987078\n",
      "[72.68415842 15.89042567]\n",
      "Gradient Descent(96/99): loss=2.65550286535086, w0=72.6841584158416, w1=15.897765580795049\n",
      "[72.68415842 15.89776558]\n",
      "Gradient Descent(97/99): loss=2.6554821444585412, w0=72.6841584158416, w1=15.90510549360302\n",
      "[72.68415842 15.90510549]\n",
      "Gradient Descent(98/99): loss=2.655461423566223, w0=72.6841584158416, w1=15.912445406410992\n",
      "[72.68415842 15.91244541]\n",
      "Gradient Descent(99/99): loss=2.655466340673134, w0=72.67128712871288, w1=15.908496935380338\n",
      "Gradient Descent: execution time=0.016 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=1, description='n_iter', max=101, min=1), Output()), _dom_classes=('widg…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca54a64f727f4caf8e9ab4805c2334d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def computeGDSub():\n",
    "    from plots import gradient_descent_visualization\n",
    "\n",
    "    # Define the parameters of the algorithm.\n",
    "    max_iters = 100\n",
    "    gamma = 1.3\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.array([0, 0])\n",
    "\n",
    "    # Start gradient descent.\n",
    "    start_time = datetime.datetime.now()\n",
    "    gradient_losses, gradient_ws = gradient_descent_sub(y, tx, w_initial, max_iters, gamma)\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    # Print result\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    def plot_figure(n_iter):\n",
    "        fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "        fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "    interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))\n",
    "computeGDSub()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "def stochastic_sub_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient descent.\n",
    "    # ***************************************************\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        small_y, small_tx = batch_iter(y, tx, batch_size).__next__()\n",
    "        small_y, small_tx = np.asarray(small_y), np.asarray(small_tx)\n",
    "        loss = compute_loss(small_y, small_tx, w)\n",
    "        gradient = compute_subgradient(small_y, small_tx, w)\n",
    "\n",
    "        w = w - gamma * gradient\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Stochastic Sub-Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Sub-Gradient Descent(0/99): loss=5022.398023257965, w0=1.3, w1=2.530824020013244\n",
      "Stochastic Sub-Gradient Descent(1/99): loss=3905.6411326161756, w0=2.6, w1=3.1770485347383\n",
      "Stochastic Sub-Gradient Descent(2/99): loss=3300.63376869048, w0=3.9000000000000004, w1=4.317175302439526\n",
      "Stochastic Sub-Gradient Descent(3/99): loss=2871.97444101601, w0=5.2, w1=5.1503992316111304\n",
      "Stochastic Sub-Gradient Descent(4/99): loss=2408.103426817334, w0=6.5, w1=4.000565657270133\n",
      "Stochastic Sub-Gradient Descent(5/99): loss=2070.1723220028553, w0=7.8, w1=3.795956583063044\n",
      "Stochastic Sub-Gradient Descent(6/99): loss=983.3326551570619, w0=9.1, w1=2.5319866817017473\n",
      "Stochastic Sub-Gradient Descent(7/99): loss=1291.1996774337736, w0=10.4, w1=1.8164589925209382\n",
      "Stochastic Sub-Gradient Descent(8/99): loss=2785.673019268923, w0=11.700000000000001, w1=2.956585760222164\n",
      "Stochastic Sub-Gradient Descent(9/99): loss=3212.279583732867, w0=13.000000000000002, w1=4.9857455725414965\n",
      "Stochastic Sub-Gradient Descent(10/99): loss=2377.110390355628, w0=14.300000000000002, w1=5.8217936726389\n",
      "Stochastic Sub-Gradient Descent(11/99): loss=1745.5808347445743, w0=15.600000000000003, w1=5.918662885765505\n",
      "Stochastic Sub-Gradient Descent(12/99): loss=1917.355862047036, w0=16.900000000000002, w1=7.677214229001447\n",
      "Stochastic Sub-Gradient Descent(13/99): loss=1709.7199167246647, w0=18.200000000000003, w1=6.9418297773037585\n",
      "Stochastic Sub-Gradient Descent(14/99): loss=1077.3658865065954, w0=19.500000000000004, w1=5.9333706347685755\n",
      "Stochastic Sub-Gradient Descent(15/99): loss=1029.1613078914638, w0=20.800000000000004, w1=5.65395109278791\n",
      "Stochastic Sub-Gradient Descent(16/99): loss=704.452467598965, w0=22.100000000000005, w1=4.852050590261558\n",
      "Stochastic Sub-Gradient Descent(17/99): loss=1254.6866544930485, w0=23.400000000000006, w1=4.227376526005987\n",
      "Stochastic Sub-Gradient Descent(18/99): loss=804.4898311814643, w0=24.700000000000006, w1=3.4108123451335013\n",
      "Stochastic Sub-Gradient Descent(19/99): loss=541.5604981297671, w0=26.000000000000007, w1=2.374296895313857\n",
      "Stochastic Sub-Gradient Descent(20/99): loss=1065.0044026294386, w0=27.300000000000008, w1=1.8999234018395112\n",
      "Stochastic Sub-Gradient Descent(21/99): loss=682.8326691514452, w0=28.60000000000001, w1=1.2492976684410704\n",
      "Stochastic Sub-Gradient Descent(22/99): loss=622.3393572449911, w0=29.90000000000001, w1=0.9115505303728997\n",
      "Stochastic Sub-Gradient Descent(23/99): loss=2678.395600868126, w0=31.20000000000001, w1=3.447236656748178\n",
      "Stochastic Sub-Gradient Descent(24/99): loss=1023.1519658187813, w0=32.50000000000001, w1=3.8727483709950143\n",
      "Stochastic Sub-Gradient Descent(25/99): loss=1345.0934228768617, w0=33.800000000000004, w1=4.237388330384152\n",
      "Stochastic Sub-Gradient Descent(26/99): loss=365.2029681931661, w0=35.1, w1=2.7621309763298454\n",
      "Stochastic Sub-Gradient Descent(27/99): loss=510.1248785486709, w0=36.4, w1=2.612047266136525\n",
      "Stochastic Sub-Gradient Descent(28/99): loss=1460.5495781559457, w0=37.699999999999996, w1=3.4290498029214342\n",
      "Stochastic Sub-Gradient Descent(29/99): loss=1349.8937478761707, w0=38.99999999999999, w1=4.788412619435613\n",
      "Stochastic Sub-Gradient Descent(30/99): loss=551.4012103775427, w0=40.29999999999999, w1=5.8068595549858335\n",
      "Stochastic Sub-Gradient Descent(31/99): loss=257.04668897530263, w0=41.59999999999999, w1=5.162275010262993\n",
      "Stochastic Sub-Gradient Descent(32/99): loss=182.8229571371118, w0=42.899999999999984, w1=4.642406881452419\n",
      "Stochastic Sub-Gradient Descent(33/99): loss=296.5185455825026, w0=44.19999999999998, w1=4.300394007432682\n",
      "Stochastic Sub-Gradient Descent(34/99): loss=859.648661225965, w0=45.49999999999998, w1=4.816915520921531\n",
      "Stochastic Sub-Gradient Descent(35/99): loss=117.44179000314321, w0=46.799999999999976, w1=3.9994100478942984\n",
      "Stochastic Sub-Gradient Descent(36/99): loss=65.44980525196263, w0=48.09999999999997, w1=2.8390372911025414\n",
      "Stochastic Sub-Gradient Descent(37/99): loss=308.91031344286023, w0=49.39999999999997, w1=2.8045325817189135\n",
      "Stochastic Sub-Gradient Descent(38/99): loss=319.9350832921835, w0=50.69999999999997, w1=3.009375501003241\n",
      "Stochastic Sub-Gradient Descent(39/99): loss=66.66861198883609, w0=51.999999999999964, w1=3.1139505003730905\n",
      "Stochastic Sub-Gradient Descent(40/99): loss=275.73948931326845, w0=53.29999999999996, w1=3.736516888793421\n",
      "Stochastic Sub-Gradient Descent(41/99): loss=554.4432711320585, w0=54.59999999999996, w1=4.910740827402275\n",
      "Stochastic Sub-Gradient Descent(42/99): loss=500.23125067254983, w0=55.899999999999956, w1=6.068647340219462\n",
      "Stochastic Sub-Gradient Descent(43/99): loss=51.1839418037289, w0=57.19999999999995, w1=4.141835265994315\n",
      "Stochastic Sub-Gradient Descent(44/99): loss=19.583522530991946, w0=58.49999999999995, w1=3.3252710851218295\n",
      "Stochastic Sub-Gradient Descent(45/99): loss=231.4405520318134, w0=59.79999999999995, w1=3.826956778516931\n",
      "Stochastic Sub-Gradient Descent(46/99): loss=10.653690619799285, w0=61.099999999999945, w1=3.5475372365362654\n",
      "Stochastic Sub-Gradient Descent(47/99): loss=18.983825657952885, w0=59.79999999999995, w1=5.746398465839206\n",
      "Stochastic Sub-Gradient Descent(48/99): loss=266.96102415824265, w0=61.099999999999945, w1=7.160347088417478\n",
      "Stochastic Sub-Gradient Descent(49/99): loss=83.38312582848816, w0=62.39999999999994, w1=7.652090862628905\n",
      "Stochastic Sub-Gradient Descent(50/99): loss=3.5190466991345217, w0=61.099999999999945, w1=8.808694189580876\n",
      "Stochastic Sub-Gradient Descent(51/99): loss=89.7792937516356, w0=62.39999999999994, w1=8.33432069610653\n",
      "Stochastic Sub-Gradient Descent(52/99): loss=74.60681231143289, w0=63.69999999999994, w1=8.870101330284589\n",
      "Stochastic Sub-Gradient Descent(53/99): loss=0.3883838554303192, w0=64.99999999999994, w1=8.084085255707063\n",
      "Stochastic Sub-Gradient Descent(54/99): loss=189.84687877471762, w0=66.29999999999994, w1=10.614909275720306\n",
      "Stochastic Sub-Gradient Descent(55/99): loss=83.2742993081443, w0=67.59999999999994, w1=9.90524673298397\n",
      "Stochastic Sub-Gradient Descent(56/99): loss=125.41267186051712, w0=68.89999999999993, w1=10.42176824647282\n",
      "Stochastic Sub-Gradient Descent(57/99): loss=133.58533880208844, w0=70.19999999999993, w1=11.653321978124193\n",
      "Stochastic Sub-Gradient Descent(58/99): loss=1.871941477359097, w0=68.89999999999993, w1=12.444067680235566\n",
      "Stochastic Sub-Gradient Descent(59/99): loss=3.497669487313646, w0=70.19999999999993, w1=10.194282316944088\n",
      "Stochastic Sub-Gradient Descent(60/99): loss=5.548584356722823, w0=71.49999999999993, w1=10.399125236228416\n",
      "Stochastic Sub-Gradient Descent(61/99): loss=29.964083194056133, w0=72.79999999999993, w1=9.249291661887419\n",
      "Stochastic Sub-Gradient Descent(62/99): loss=12.12636403809156, w0=74.09999999999992, w1=8.599406931995402\n",
      "Stochastic Sub-Gradient Descent(63/99): loss=60.74648990120622, w0=72.79999999999993, w1=10.35163325856434\n",
      "Stochastic Sub-Gradient Descent(64/99): loss=9.558691190959227, w0=74.09999999999992, w1=10.815173260697671\n",
      "Stochastic Sub-Gradient Descent(65/99): loss=1.6147090522880052, w0=72.79999999999993, w1=11.470959656465542\n",
      "Stochastic Sub-Gradient Descent(66/99): loss=11.224446369949234, w0=71.49999999999993, w1=12.287523837338027\n",
      "Stochastic Sub-Gradient Descent(67/99): loss=52.930580202439415, w0=72.79999999999993, w1=15.147639608628515\n",
      "Stochastic Sub-Gradient Descent(68/99): loss=28.897935537588186, w0=74.09999999999992, w1=14.700105481390453\n",
      "Stochastic Sub-Gradient Descent(69/99): loss=21.384939356069935, w0=75.39999999999992, w1=12.792945685861463\n",
      "Stochastic Sub-Gradient Descent(70/99): loss=37.837594941476404, w0=74.09999999999992, w1=14.293728347796481\n",
      "Stochastic Sub-Gradient Descent(71/99): loss=30.51394056260583, w0=72.79999999999993, w1=15.330243797616125\n",
      "Stochastic Sub-Gradient Descent(72/99): loss=0.03210975664122528, w0=71.49999999999993, w1=14.90473208336929\n",
      "Stochastic Sub-Gradient Descent(73/99): loss=0.004723511833686413, w0=72.79999999999993, w1=13.230272163745596\n",
      "Stochastic Sub-Gradient Descent(74/99): loss=13.581129663290112, w0=71.49999999999993, w1=14.750637707670334\n",
      "Stochastic Sub-Gradient Descent(75/99): loss=5.514753310190112, w0=70.19999999999993, w1=15.688527781058362\n",
      "Stochastic Sub-Gradient Descent(76/99): loss=0.9559337308963148, w0=71.49999999999993, w1=13.597510441699043\n",
      "Stochastic Sub-Gradient Descent(77/99): loss=3.1785493715701643, w0=70.19999999999993, w1=12.377028278622642\n",
      "Stochastic Sub-Gradient Descent(78/99): loss=11.95556803546625, w0=71.49999999999993, w1=11.291876423180273\n",
      "Stochastic Sub-Gradient Descent(79/99): loss=53.379114077306426, w0=72.79999999999993, w1=12.625608310666767\n",
      "Stochastic Sub-Gradient Descent(80/99): loss=35.22762153495068, w0=74.09999999999992, w1=13.652990726236691\n",
      "Stochastic Sub-Gradient Descent(81/99): loss=9.432518391370875, w0=75.39999999999992, w1=14.810897239053878\n",
      "Stochastic Sub-Gradient Descent(82/99): loss=5.727368072656124, w0=76.69999999999992, w1=15.094842787980959\n",
      "Stochastic Sub-Gradient Descent(83/99): loss=3.762080208896386, w0=75.39999999999992, w1=15.77440985933741\n",
      "Stochastic Sub-Gradient Descent(84/99): loss=19.852868362561733, w0=76.69999999999992, w1=17.96733287492293\n",
      "Stochastic Sub-Gradient Descent(85/99): loss=14.885039897152732, w0=75.39999999999992, w1=17.955395381863145\n",
      "Stochastic Sub-Gradient Descent(86/99): loss=34.17856575555051, w0=76.69999999999992, w1=16.582348754324393\n",
      "Stochastic Sub-Gradient Descent(87/99): loss=26.117671661440614, w0=75.39999999999992, w1=17.74272151111615\n",
      "Stochastic Sub-Gradient Descent(88/99): loss=23.957652774952706, w0=74.09999999999992, w1=17.12015512269582\n",
      "Stochastic Sub-Gradient Descent(89/99): loss=0.032251233726303744, w0=72.79999999999993, w1=15.53002845375882\n",
      "Stochastic Sub-Gradient Descent(90/99): loss=0.6514962276489201, w0=71.49999999999993, w1=16.33047686829196\n",
      "Stochastic Sub-Gradient Descent(91/99): loss=0.568441141576398, w0=72.79999999999993, w1=17.314207248693034\n",
      "Stochastic Sub-Gradient Descent(92/99): loss=2.8378059455830535, w0=74.09999999999992, w1=18.472113761510222\n",
      "Stochastic Sub-Gradient Descent(93/99): loss=8.794798660762059, w0=72.79999999999993, w1=16.395251923231758\n",
      "Stochastic Sub-Gradient Descent(94/99): loss=43.735580244810194, w0=74.09999999999992, w1=15.653401774513826\n",
      "Stochastic Sub-Gradient Descent(95/99): loss=6.8402025077438156, w0=72.79999999999993, w1=14.553382703587209\n",
      "Stochastic Sub-Gradient Descent(96/99): loss=4.553705012927947, w0=74.09999999999992, w1=14.493157403447826\n",
      "Stochastic Sub-Gradient Descent(97/99): loss=66.72639286829447, w0=72.79999999999993, w1=15.757127304809122\n",
      "Stochastic Sub-Gradient Descent(98/99): loss=5.804868975708043, w0=74.09999999999992, w1=16.9018459240783\n",
      "Stochastic Sub-Gradient Descent(99/99): loss=39.14023938147224, w0=72.79999999999993, w1=16.115269311263702\n",
      "Gradient Descent: execution time=0.016 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=1, description='n_iter', max=101, min=1), Output()), _dom_classes=('widg…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad57dfc881ff483a9264773a9826ca1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def computeSGDSub():\n",
    "    from plots import gradient_descent_visualization\n",
    "\n",
    "    # Define the parameters of the algorithm.\n",
    "    max_iters = 100\n",
    "    gamma = 1.3\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.array([0, 0])\n",
    "\n",
    "    # Start gradient descent.\n",
    "    start_time = datetime.datetime.now()\n",
    "    gradient_losses, gradient_ws = stochastic_sub_gradient_descent(y, tx, w_initial, 1, max_iters, gamma)\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    # Print result\n",
    "    exection_time = (end_time - start_time).total_seconds()\n",
    "    print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "    def plot_figure(n_iter):\n",
    "        fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "        fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "    interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))\n",
    "computeSGDSub()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}